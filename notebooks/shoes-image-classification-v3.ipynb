{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## <b>Importing Libraries:</b>","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install -Uqq timm","metadata":{"execution":{"iopub.status.busy":"2022-12-09T12:42:11.000813Z","iopub.execute_input":"2022-12-09T12:42:11.001539Z","iopub.status.idle":"2022-12-09T12:42:20.380169Z","shell.execute_reply.started":"2022-12-09T12:42:11.001503Z","shell.execute_reply":"2022-12-09T12:42:20.378814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install fastai","metadata":{"execution":{"iopub.status.busy":"2022-12-09T12:43:09.073007Z","iopub.execute_input":"2022-12-09T12:43:09.073453Z","iopub.status.idle":"2022-12-09T12:43:18.592368Z","shell.execute_reply.started":"2022-12-09T12:43:09.073408Z","shell.execute_reply":"2022-12-09T12:43:18.591118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import timm\nfrom pathlib import Path\nfrom fastai.vision.all import *\nfrom fastai.vision.all import vision_learner, get_image_files","metadata":{"execution":{"iopub.status.busy":"2022-12-09T12:43:18.596452Z","iopub.execute_input":"2022-12-09T12:43:18.596778Z","iopub.status.idle":"2022-12-09T12:43:18.605809Z","shell.execute_reply.started":"2022-12-09T12:43:18.596748Z","shell.execute_reply":"2022-12-09T12:43:18.604754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = Path(\"/kaggle/input/shoes-classification-dataset-13k-images/Shoes Dataset\")\npath.ls()","metadata":{"execution":{"iopub.status.busy":"2022-12-09T11:11:08.624796Z","iopub.execute_input":"2022-12-09T11:11:08.625715Z","iopub.status.idle":"2022-12-09T11:11:08.640988Z","shell.execute_reply.started":"2022-12-09T11:11:08.625678Z","shell.execute_reply":"2022-12-09T11:11:08.639866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b>Data Exploration:</b>","metadata":{}},{"cell_type":"code","source":"trn_path = path/'Train'\nfiles = get_image_files(trn_path)","metadata":{"execution":{"iopub.status.busy":"2022-12-09T11:11:12.404260Z","iopub.execute_input":"2022-12-09T11:11:12.404616Z","iopub.status.idle":"2022-12-09T11:11:19.789550Z","shell.execute_reply.started":"2022-12-09T11:11:12.404586Z","shell.execute_reply":"2022-12-09T11:11:19.788545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = PILImage.create(files[0])\nprint(img.size)\nimg.to_thumb(128)","metadata":{"execution":{"iopub.status.busy":"2022-12-09T09:48:22.544059Z","iopub.execute_input":"2022-12-09T09:48:22.544802Z","iopub.status.idle":"2022-12-09T09:48:22.584198Z","shell.execute_reply.started":"2022-12-09T09:48:22.544736Z","shell.execute_reply":"2022-12-09T09:48:22.583126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Checking the sizes of images in the dataset","metadata":{}},{"cell_type":"code","source":"from fastcore.parallel import *\n\ndef f(o): return PILImage.create(o).size\nsizes = parallel(f, files, n_workers=8)\npd.Series(sizes).value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-12-09T09:48:22.588115Z","iopub.execute_input":"2022-12-09T09:48:22.590335Z","iopub.status.idle":"2022-12-09T09:48:48.312824Z","shell.execute_reply.started":"2022-12-09T09:48:22.590299Z","shell.execute_reply":"2022-12-09T09:48:48.311823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b>Data Pre-processing:</b>","metadata":{}},{"cell_type":"code","source":"trn_path = Path('/sml')","metadata":{"execution":{"iopub.status.busy":"2022-12-09T11:11:25.592147Z","iopub.execute_input":"2022-12-09T11:11:25.592493Z","iopub.status.idle":"2022-12-09T11:11:25.597510Z","shell.execute_reply.started":"2022-12-09T11:11:25.592463Z","shell.execute_reply":"2022-12-09T11:11:25.596292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resize_images(path/'Train',dest=trn_path,max_size=224,recurse=True)#resizing all the images in the folder","metadata":{"execution":{"iopub.status.busy":"2022-12-09T11:11:27.387436Z","iopub.execute_input":"2022-12-09T11:11:27.387799Z","iopub.status.idle":"2022-12-09T11:12:27.986635Z","shell.execute_reply.started":"2022-12-09T11:11:27.387768Z","shell.execute_reply":"2022-12-09T11:12:27.985434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dls = ImageDataLoaders.from_folder(trn_path, valid_pct=0.2, seed=42,item_tfms=Resize((224,168)))\ndls.show_batch(max_n=3)","metadata":{"execution":{"iopub.status.busy":"2022-12-09T09:50:04.463095Z","iopub.execute_input":"2022-12-09T09:50:04.465498Z","iopub.status.idle":"2022-12-09T09:50:10.107769Z","shell.execute_reply.started":"2022-12-09T09:50:04.465458Z","shell.execute_reply":"2022-12-09T09:50:10.106764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#function to help us minimise the workflow\ndef train(arch, item, batch, epochs=5):\n    dls = ImageDataLoaders.from_folder(trn_path, seed=42, valid_pct=0.2, item_tfms=item, batch_tfms=batch)\n    metrics=[accuracy]\n    learn = vision_learner(dls, arch, metrics=metrics).to_fp16() #loss function is cross entropy ,default\n    learn.fine_tune(epochs, 0.01)\n    return learn","metadata":{"execution":{"iopub.status.busy":"2022-12-09T11:51:06.349964Z","iopub.execute_input":"2022-12-09T11:51:06.350412Z","iopub.status.idle":"2022-12-09T11:51:06.361030Z","shell.execute_reply.started":"2022-12-09T11:51:06.350375Z","shell.execute_reply":"2022-12-09T11:51:06.359900Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **Using Transfer Learning to classify the images in our dataset**\n- **Also implement the models alongside various data augmentation techniques**\n- **We will try different architechtures and choose depending on the results produced**","metadata":{}},{"cell_type":"markdown","source":"### <b>Model:Resnet26d</b>","metadata":{}},{"cell_type":"code","source":"arch = \"resnet26d\"","metadata":{"execution":{"iopub.status.busy":"2022-12-09T09:50:10.124116Z","iopub.execute_input":"2022-12-09T09:50:10.125690Z","iopub.status.idle":"2022-12-09T09:50:10.133637Z","shell.execute_reply.started":"2022-12-09T09:50:10.125653Z","shell.execute_reply":"2022-12-09T09:50:10.132667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### <b>Pre-processing Experiments:</b>","metadata":{}},{"cell_type":"code","source":"learn = vision_learner(dls,\"resnet26d\", metrics=[accuracy]).to_fp16()","metadata":{"execution":{"iopub.status.busy":"2022-12-09T09:50:10.135322Z","iopub.execute_input":"2022-12-09T09:50:10.136163Z","iopub.status.idle":"2022-12-09T09:50:15.799735Z","shell.execute_reply.started":"2022-12-09T09:50:10.136119Z","shell.execute_reply":"2022-12-09T09:50:15.798699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.lr_find() #finding the learning rate","metadata":{"execution":{"iopub.status.busy":"2022-12-09T09:50:15.804264Z","iopub.execute_input":"2022-12-09T09:50:15.805192Z","iopub.status.idle":"2022-12-09T09:50:49.453386Z","shell.execute_reply.started":"2022-12-09T09:50:15.805154Z","shell.execute_reply":"2022-12-09T09:50:49.452319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train(arch,item=Resize(224,method='squish'),batch=aug_transforms(size=224,min_scale=0.75)) #squish the images","metadata":{"execution":{"iopub.status.busy":"2022-12-09T09:50:49.458156Z","iopub.execute_input":"2022-12-09T09:50:49.460418Z","iopub.status.idle":"2022-12-09T09:56:44.296796Z","shell.execute_reply.started":"2022-12-09T09:50:49.460380Z","shell.execute_reply":"2022-12-09T09:56:44.295532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- This gives our model an accuracy of 87 percent. ","metadata":{}},{"cell_type":"code","source":"train(arch,item=Resize(224),batch=aug_transforms(size=224,min_scale=0.75)) #crop the images","metadata":{"execution":{"iopub.status.busy":"2022-12-09T09:56:44.301878Z","iopub.execute_input":"2022-12-09T09:56:44.305949Z","iopub.status.idle":"2022-12-09T10:02:27.925062Z","shell.execute_reply.started":"2022-12-09T09:56:44.305902Z","shell.execute_reply":"2022-12-09T10:02:27.923834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- While the accuracy seems to have a slight improvement , the test time augmentation error is slightly higher than the previous one","metadata":{}},{"cell_type":"code","source":"learn = train(arch, item=Resize((224,168), method=ResizeMethod.Pad, pad_mode=PadMode.Zeros),\n              batch=aug_transforms(size=(168,128), min_scale=0.75)) #padding the images and resizing","metadata":{"execution":{"iopub.status.busy":"2022-12-09T10:02:27.929982Z","iopub.execute_input":"2022-12-09T10:02:27.932368Z","iopub.status.idle":"2022-12-09T10:07:39.976775Z","shell.execute_reply.started":"2022-12-09T10:02:27.932327Z","shell.execute_reply":"2022-12-09T10:07:39.975538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Resizing the images to a lower size does seem to have an impact on the model","metadata":{}},{"cell_type":"markdown","source":"### <b>Model:convnext_small_in22k</b>","metadata":{}},{"cell_type":"code","source":"arch = 'convnext_small_in22k'","metadata":{"execution":{"iopub.status.busy":"2022-12-09T10:07:39.981858Z","iopub.execute_input":"2022-12-09T10:07:39.984302Z","iopub.status.idle":"2022-12-09T10:07:39.991412Z","shell.execute_reply.started":"2022-12-09T10:07:39.984261Z","shell.execute_reply":"2022-12-09T10:07:39.990087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### <b>Pre-processing Experiments:</b>","metadata":{}},{"cell_type":"code","source":"train(arch,item=Resize(224),batch=aug_transforms(size=224,min_scale=0.75)) #cropping the images and square like images","metadata":{"execution":{"iopub.status.busy":"2022-12-09T10:07:39.992711Z","iopub.execute_input":"2022-12-09T10:07:39.993297Z","iopub.status.idle":"2022-12-09T10:19:20.780146Z","shell.execute_reply.started":"2022-12-09T10:07:39.993216Z","shell.execute_reply":"2022-12-09T10:19:20.779127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- ConvNext seems to perform way better,gives us improved results.","metadata":{}},{"cell_type":"code","source":"learn = train(arch,item=Resize((224,168),method='squish'),batch=aug_transforms(size=224,min_scale=0.75))\n#squish the images and rectangle images","metadata":{"execution":{"iopub.status.busy":"2022-12-09T10:19:20.781886Z","iopub.execute_input":"2022-12-09T10:19:20.782256Z","iopub.status.idle":"2022-12-09T10:31:09.349512Z","shell.execute_reply.started":"2022-12-09T10:19:20.782215Z","shell.execute_reply":"2022-12-09T10:31:09.348427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train(arch, item=Resize((224,168), method=ResizeMethod.Pad, pad_mode=PadMode.Zeros),\n              batch=aug_transforms(size=(168,128), min_scale=0.75)) #padding the images and rectangle like images","metadata":{"execution":{"iopub.status.busy":"2022-12-09T10:31:09.351611Z","iopub.execute_input":"2022-12-09T10:31:09.352026Z","iopub.status.idle":"2022-12-09T10:37:14.840955Z","shell.execute_reply.started":"2022-12-09T10:31:09.351987Z","shell.execute_reply":"2022-12-09T10:37:14.839557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Padding the images seems to have lesser accuracy compared to cropping the images.","metadata":{}},{"cell_type":"markdown","source":"### <b>Model:convnext_tiny_hnf</b>","metadata":{}},{"cell_type":"code","source":"arch =\"convnext_tiny_hnf\"","metadata":{"execution":{"iopub.status.busy":"2022-12-09T10:37:14.842738Z","iopub.execute_input":"2022-12-09T10:37:14.843490Z","iopub.status.idle":"2022-12-09T10:37:14.848732Z","shell.execute_reply.started":"2022-12-09T10:37:14.843447Z","shell.execute_reply":"2022-12-09T10:37:14.847560Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### <b>Pre-processing Experiments:</b>","metadata":{}},{"cell_type":"code","source":"train(arch,item=Resize(224),batch=aug_transforms(size=224,min_scale=0.75))","metadata":{"execution":{"iopub.status.busy":"2022-12-09T10:37:14.850466Z","iopub.execute_input":"2022-12-09T10:37:14.850862Z","iopub.status.idle":"2022-12-09T10:45:55.618095Z","shell.execute_reply.started":"2022-12-09T10:37:14.850823Z","shell.execute_reply":"2022-12-09T10:45:55.617077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **From our experiments above we are able to see ConvNext models seems to perform better compared to Resnets.**\n- **Cropping the images gives much better performance compared to padding or squishing the images when trying to classify**","metadata":{}},{"cell_type":"markdown","source":"### <b>Choosing the Final Model:</b>\n- From our above experiments we can choose the **convnext_small_in22k** as the model to classify images","metadata":{}},{"cell_type":"code","source":"arch = 'convnext_small_in22k'","metadata":{"execution":{"iopub.status.busy":"2022-12-09T11:13:31.339405Z","iopub.execute_input":"2022-12-09T11:13:31.339763Z","iopub.status.idle":"2022-12-09T11:13:31.344283Z","shell.execute_reply.started":"2022-12-09T11:13:31.339733Z","shell.execute_reply":"2022-12-09T11:13:31.343182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We will increase the size of the images leading to better Image classification.","metadata":{}},{"cell_type":"code","source":"learn = train(arch,epochs=10,item=Resize((360,320), method=ResizeMethod.Pad, pad_mode=PadMode.Zeros),\n              batch=aug_transforms(size=(275,275), min_scale=0.75)) ","metadata":{"execution":{"iopub.status.busy":"2022-12-09T11:51:40.244394Z","iopub.execute_input":"2022-12-09T11:51:40.244892Z","iopub.status.idle":"2022-12-09T12:20:21.101924Z","shell.execute_reply.started":"2022-12-09T11:51:40.244812Z","shell.execute_reply":"2022-12-09T12:20:21.100857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **Progressive resizing has improved the accuracy of our model!**","metadata":{}},{"cell_type":"markdown","source":"### <b>Interpreting Results of Model:</b>","metadata":{}},{"cell_type":"code","source":"learn.show_results()","metadata":{"execution":{"iopub.status.busy":"2022-12-09T12:21:43.246339Z","iopub.execute_input":"2022-12-09T12:21:43.246721Z","iopub.status.idle":"2022-12-09T12:21:44.744349Z","shell.execute_reply.started":"2022-12-09T12:21:43.246689Z","shell.execute_reply":"2022-12-09T12:21:44.743096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()","metadata":{"execution":{"iopub.status.busy":"2022-12-09T12:20:30.933914Z","iopub.execute_input":"2022-12-09T12:20:30.934572Z","iopub.status.idle":"2022-12-09T12:20:59.025392Z","shell.execute_reply.started":"2022-12-09T12:20:30.934527Z","shell.execute_reply":"2022-12-09T12:20:59.022483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"interp.plot_top_losses(6,figsize = (25,5))","metadata":{"execution":{"iopub.status.busy":"2022-12-09T12:22:00.761680Z","iopub.execute_input":"2022-12-09T12:22:00.762079Z","iopub.status.idle":"2022-12-09T12:22:01.485798Z","shell.execute_reply.started":"2022-12-09T12:22:00.762028Z","shell.execute_reply":"2022-12-09T12:22:01.484802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b>Prediction using the Trained Model:</b>\n- Let us randomly input a image from the test dataset to see how it predicts","metadata":{}},{"cell_type":"code","source":"learn.predict('/kaggle/input/shoes-classification-dataset-13k-images/Shoes Dataset/Test/Clog/Clog-Test (104).jpeg')","metadata":{"execution":{"iopub.status.busy":"2022-12-09T12:23:23.957919Z","iopub.execute_input":"2022-12-09T12:23:23.958317Z","iopub.status.idle":"2022-12-09T12:23:24.054880Z","shell.execute_reply.started":"2022-12-09T12:23:23.958283Z","shell.execute_reply":"2022-12-09T12:23:24.053861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **Gives us a accurate prediction!!**","metadata":{}},{"cell_type":"markdown","source":"### **Saving and Loading the Model:**","metadata":{}},{"cell_type":"code","source":"learn.export(Path(\"/kaggle/working/export2.pkl\"))\nlearn.model_dir = \"/kaggle/working\"","metadata":{"execution":{"iopub.status.busy":"2022-12-09T12:25:16.400356Z","iopub.execute_input":"2022-12-09T12:25:16.400742Z","iopub.status.idle":"2022-12-09T12:25:16.811426Z","shell.execute_reply.started":"2022-12-09T12:25:16.400707Z","shell.execute_reply":"2022-12-09T12:25:16.810427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#storing the requirements needed to run the model\n!pip freeze > /kaggle/working/requirements.txt","metadata":{"execution":{"iopub.status.busy":"2022-12-09T12:43:58.632727Z","iopub.execute_input":"2022-12-09T12:43:58.633439Z","iopub.status.idle":"2022-12-09T12:44:01.970376Z","shell.execute_reply.started":"2022-12-09T12:43:58.633400Z","shell.execute_reply":"2022-12-09T12:44:01.969059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Thanks for viewing the notebook!**","metadata":{}}]}